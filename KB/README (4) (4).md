# ðŸ“š Assignment Q&A Agent using Amazon Bedrock Knowledge Base

This project deploys a fast, accurate, and grounded **Assignment Assistant** on AWS Lambda using **Amazon Bedrock Knowledge Base (KB)** and **Titan Text Express**.

It allows students to ask natural language questions about assignment deadlines, submission instructions, rubrics, and requirements. The answers are generated based on your own curated documents (PDFs, Markdown, etc.) ingested into a Bedrock KB.

---

## ðŸš€ Features

- âœ… Serverless Lambda using Bedrock Knowledge Base
- âš¡ Fast generation using Titan Text Express
- ðŸ“Ž Accurate answers grounded in your uploaded documents
- ðŸ§¬ Embedded vector search using Titan Embeddings
- ðŸ” IAM secure API access

---

## ðŸ§ª Example Questions

- *When is the secure coding assignment due?*
- *How do I submit the database assignment?*
- *What are the grading criteria?*
- *Can I submit after the deadline?*

---

## ðŸ“ Folder Structure

```
assignment_agent_lambda/
â”œâ”€â”€ lambda_function.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ retrieval.py
â”‚   â””â”€â”€ prompt_templates.py
```

---

## ðŸ› ï¸ Setup Instructions

### 1. Create a Knowledge Base

- In the AWS Console > Amazon Bedrock > Knowledge Bases
- Upload assignment documents (PDF, DOCX, TXT, Markdown)
- Choose **Titan Embeddings**: `amazon.titan-embed-text-v1`
- Name your KB and note the `knowledgeBaseId`

### 2. Deploy Lambda

Set up a Lambda function using `lambda_function.py`. Add environment variables:

- `KNOWLEDGE_BASE_ID`: your KBâ€™s ID
- `MODEL_ARN`: e.g. `arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1`

### 3. Add IAM Permissions

Attach this to your Lambda role:

```json
{
  "Effect": "Allow",
  "Action": "bedrock-agent:RetrieveAndGenerate",
  "Resource": "*"
}
```

---

## ðŸ“¥ Sample Input

```json
{
  "question": "How do I submit the secure coding assignment?"
}
```

## ðŸ“¤ Sample Output

```json
{
  "question": "How do I submit the secure coding assignment?",
  "answer": "You must submit it via Moodle before Sept 20, 11:59 PM using the PDF upload link."
}
```

---

## ðŸ”„ End-to-End Flow

### ðŸ” Assignment Agent Runtime Flow

```
[Student Question]
     â†“
[Lambda â†’ retrieve_and_generate()]
     â†“
[Retrieve: vector search in KB]
     â†“
[Generate: Titan Text Express]
     â†“
[Return grounded answer]
```

---

### ðŸ§¬ Embeddings Flow

#### ðŸ“¥ When Creating Knowledge Base

```
Documents â†’ Chunked â†’ Embedded (Titan) â†’ Vector DB
```

- Embeddings are only created **once** during ingestion
- Uses `amazon.titan-embed-text-v1`

#### ðŸ¤– When Answering Questions

```
Question â†’ Embedded (Titan) â†’ Match vectors â†’ Generate with Titan Text Express
```

- Same embedding model used for matching (automatically handled)
- Generation model (e.g., `titan-text-express-v1`) specified at runtime

---

## âš¡ Recommended Models for Speed

| Use Case       | Model                           | ARN |
|----------------|----------------------------------|-----|
| Fast Q&A       | `amazon.titan-text-express-v1`  | `arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1` |

---

## ðŸ§  Notes

- **Embeddings and generation are separate.**
  - Embeddings = used to index and search your KB.
  - Generation = used to create natural language answers.
- The embedding model is fixed at KB creation.
- The generation model ARN is chosen per API call.

---

## ðŸ“… Last updated: June 28, 2025


---

## âœ¨ Prompt Design & Tuning Guide

While Amazon Bedrock handles grounding via its Knowledge Base, the way you **frame the prompt** for the language model still affects the final output clarity and accuracy.

### âœ… Best Practice Prompt Template

```text
You are a helpful academic assistant. Answer student questions using only the information in the provided assignment context.

If the answer is not in the context, respond with:
"I donâ€™t know. Please check the official assignment brief or ask your instructor."

Context:
{{retrieved_context}}

Student Question:
{{user_question}}

Answer:
```

- This prompt is automatically handled internally by Bedrock KB unless you're using a custom orchestration layer.
- You can override this format using `promptOverrideConfiguration` in advanced use cases.

---

## ðŸ§ª Prompt Testing Strategies

Use a range of test questions to evaluate the reliability of the agent:

| Test Type       | Sample Question                                 | Expectation                           |
|------------------|--------------------------------------------------|----------------------------------------|
| âœ… Factual match  | *When is the secure coding assignment due?*     | Correct date, grounded in KB           |
| âš ï¸ Not in KB      | *Can I submit via email?*                      | "I donâ€™t know" or clarification needed |
| âœ… Rubric query   | *How are marks awarded?*                        | Bullet list or rubric description      |
| âš ï¸ Ambiguous      | *What do I do?*                                | Ask for clarification or return N/A    |
| âœ… Submission     | *Where do I upload my assignment?*              | "Moodle", "Turnitin", etc.             |

> Use CloudWatch Logs to inspect returned answers for various test questions. Ensure responses are grounded in your KB documents and that irrelevant queries fail gracefully.

---

## ðŸ§° Advanced Prompt Overrides (Optional)

If you implement your own Bedrock orchestration (e.g., with LangChain or Prompt Engineering), you can:

- Add step-by-step reasoning
- Format the response as JSON or Markdown
- Include confidence levels

Example advanced prompt:

```text
You are an AI tutor. Use ONLY the context provided below. Show reasoning if applicable. If unsure, say "I donâ€™t know."

Context:
{{retrieved_context}}

Question:
{{user_question}}

Answer (with reasoning):
```

---
